\documentclass[10pt]{article}
\usepackage{makeidx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{ulem}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\author{Juan Francisco Zamora Osorio}
\title{ADDITIONAL INFORMATION POSTDOCTORADO 2017}
\usepackage[paperwidth=612pt,paperheight=792pt,top=28pt,right=34pt,bottom=35pt,left=34pt]{geometry}

\begin{document}


\textbf{{\large ADDITIONAL INFORMATION:}}

-Summary of your Doctoral Thesis

-Sponsoring Researcher Choice Justification

The maximum length for this section is 1 page (Verdana font size 10, letter size
is suggested)

\section{Summary of Doctoral Thesis}
%\section{Resumen de la tesis doctoral del Investigador Responsable}
The increasing use of Internet and the wide spread of information technologies for textual content generation in news media and social networks have enabled the production of large volumes of text data. Since these large amounts of data do not generally fit in the main memory and the usage of secondary storage results prohibitive due to the high latencies involved, special techniques to process and extract valuable information from these massive volumes of text are needed.
%El uso creciente de la Internet y la amplia propagación de tecnologías de la información para la generación de contenido textual en medios de noticias y redes sociales han posibilitado la producción de grandes volúmenes de texto. Debido a que generalmente estas cantidades de datos no caben en la memoria principal de los computadores usados actualmente y a que el uso de memoria secundaria resulta prohibitivo por sus altas latencias, se necesitan técnicas especiales para procesar y extraer información valiosa a partir de estos volúmenes masivos de texto.

Traditional clustering algorithms have problems to operate over text data whose computational representation is high dimensional and also when the document collections do not fit in main memory. An approach to deal with these issues consists in using clustering techniques capable of generating an output by making a single pass or scan on each input document. In this work, we propose a single-pass text clustering method capable of dealing with high dimensional and massive document collections under limited main memory space and secondary memory access constraints. To that end, we propose a summary data structure or sketch that allows to maintain reduced versions of the input documents and two efficient similarity estimation procedures that consider only the pairwise computation between near documents. The proposed sketch exploits the properties of two special families of functions that perform a single pass processing over each document and then obtain a reduced version of its computational representation. Additionally, this data structure enables a fast construction of a sparse similarity graph of documents, from which the final clusters are extracted by a repeated bisection method. Without the proposed technique, the construction of this (dense) document graph would involve the computation of similarity between every pair of documents within the collection, which would require a quadratic number of operations together with a storage of all document vectors in main memory. In contrast, a single-scan processing of text collections makes the information extraction task feasible on very large databases by solely performing operations in main memory. In turn, the similarity estimation only between near documents, allows to avoid several expensive operations between high dimensional vectors.
%Los métodos tradicionales de agrupamiento tiene problemas para operar sobre texto cuya representación computacional es altamente dimensional y además cuando la colección completa no cabe en memoria principal. Un enfoque para lidiar con estas dificultades consiste en usar técnicas capaces de generar un resultado realizando una sola pasada sobre cada dato o documento. Esto significa que solamente un vector representando a un documento se encuentra cargado en memoria principal a la vez, se extrae lo que se necesite de él y luego se descarta de la memoria. En este trabajo, se propone un método de clustering de una sola pasada capaz de operar eficientemente sobre datos con representaciones altamente dimensionales y de volúmenes masivos bajo restricciones de cantidad de memoria principal y acceso a memoria secundaria. Para este fin, se propone el uso de una estructura de datos que permite mantener versiones reducidas en tamaño de los datos originales y que además permite la estimación de similitud entre todos ellos mediante dos procedimientos eficientes también propuestos en este trabajo. Esta estructura resumen que se propone aprovecha las propiedades teóricas y de eficiencia de dos familias de funciones que permiten el procesamiento de una pasada sobre cada documento y la reducción de su uso de memoria, además de disminuir la cantidad de pares de documentos sobre los cuales se calcula su similitud. Adicionalmente, esta estructura de datos permite una construcción rapida de una red de documentos poco conexa, a partir de la cual se extraeran los grupos finales de documentos usando un método de disección. En ausencia del método propuesto, la construcción de esta red de documentos también sería posible, pero implicaría un alto costo computacional y de memoria, dado que la cantidad de cálculos de similitud entre pares de documentos y conexiones sería muy alta (cuadrática en la cantidad de documentos de la colección), además de tener que mantener todos los vectores asociados a los documentos cargados en memoria principal. En contraste con esto último, un procesamiento de la colección en una sola pasada hace factible la tarea de extracción de información sobre grandes bases de datos documentales, usando únicamente operaciones en memoria principal. En consecuencia, dado que la estimación de similitud no se realiza sobre todos pares de documentos (solamente entre los que probablemente son cercanos) y además que esta estimación se realiza sobre versiones reducidas de estos, este método permite procesar grandes volúmenes de datos altamente dimensionales.

In order to evaluate this proposal, real text collections extracted from different domains (news media, technical reports, government reports and medical abstracts) are employed. Finally, the obtained performance is compared against a similar algorithm that builds a document graph by using exact similarities and then partitions it. Besides the attained performance scores, it is shown that a significant reduction of the original space dimensionality (up to $95\%$) is achieved without affecting the quality of the defined clusters.
%Para evaluar esta propuesta, se usaran colecciones de texto reales generadas en distintos dominios (noticias, reportes técnicos, reglamentos gubernamentales y resúmenes de artículos científicos). Finalmente, el desempe\~no obtenido será comparado con el obtenido por algoritmo de disección de una red de documentos construída con similitudes exactas entre todos los documentos. Además de los desempeños alcanzados, se muestra que es posible alcanzar una disminución significativa en el espacio usado por los datos originales al emplear sus versiones reducidas, sin afectar la notoriamente la calidad de los agrupamientos obtenidos en varias colecciones documentales.

\section{Sponsoring Researcher choice justification}
%\section{Justificación de la elección del Investigador Patrocinante}
%El área de investigación del Dr. Allende-Cid es aprendizaje automático en ambientes con datos cambiantes y distribuidos. Específicamente, ha realizado contribuciones en el problema de regresión sobre bases de datos distribuidas, usando vecindarios de máquinas establecidos mediante medidas de similitud entre las leyes de probabilidad subyacente de estas fuentes de datos. Durante los dos últimos a\~nos, el Dr. Allende-Cid ha generado 4 trabajos en revistas de corriente principal (ISI) y además se adjudicó un proyecto de investigación financiado por CONICYT el a\~no 2015. Otro aspecto relevante en el desempe\~no de este investigador es su colaboración en un proyecto internacional con un grupo de investigación de Sistemas Distribuídos de la Universidade Federal de Alagoas (Brasil).

%El Dr. Allende-Cid es uno de los pocos especialistas nacionales en el procesamiento de datos distribuidos, área hacia la que deseo extender mi línea de investigación. Por lo tanto, espero que como resultado de esta colaboración se genere sinergia, la cual tenga como consecuencia final el desarrollo de una nueva línea de investigación en el país.


The main research activities of Dr. Allende-Cid are focused on automatic learning of algorithms in dynamic environments with distributed data. Specifically, he has contributed in the regression problem over distributed data collections, using machine neighborhoods built by using similarity measures between the estimated probability densities underlying he different data sources. In the last two years Dr. Allende-Cid has produced four articles published in indexed journals (ISI). Dr.
Allende-Cid also started a project for initiation in research funded by FONDECYT (The National Fund for Scientific and Technological Development) at 2015. Allende-Cid also engaged in collaboration with the Distributed Systems research team at the \textit{Universidade Federal de Alagoas} in Brazil.

It is very important to highlight that Dr. Allende-Cid is one of the few national research experts in distributed data processing, which is the field onto which I wish to guide my research. Therefore I expect as a result of this collaboration an powerful sinergy that ease the development of a new research line in Chile.
\end{document}


