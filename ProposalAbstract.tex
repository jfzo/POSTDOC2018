\documentclass[10pt]{article}
\usepackage{makeidx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{ulem}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\author{Juan Francisco Zamora Osorio}
\usepackage[utf8]{inputenc}
\title{PROPOSAL ABSTRACT POSTDOCTORADO 2017}
\usepackage[paperwidth=612pt,paperheight=792pt,top=28pt,right=34pt,bottom=35pt,left=34pt]{geometry}


\begin{document}

\noindent \textbf{PROPOSAL ABSTRACT: }

\noindent \textbf{}

\begin{tabular}{|p{2.3in}|p{4.7in}|}
\hline
\parbox{2.3in}{\centering }
& \parbox{4.7in} {\centering }  \\

\parbox{2.3in} {\raggedright \textbf{Name of Principal Investigator:} }
& \parbox{4.7in} {\centering }  Juan Francisco Zamora Osorio\\ 

\parbox{2.3in}{\centering }
& \parbox{4.7in} {\centering }  \\
\hline 
\parbox{2.3in}{\centering }
& \parbox{4.7in} {\centering }  \\
\parbox{2.3in} {\raggedright\textbf{Proposal Title:} }
& \parbox{4.7in} {\centering }  Clustering  Distributed and High Dimensional Document Collections\\
\parbox{2.3in}{\centering }
& \parbox{4.7in} {\centering }  \\
\hline 
\end{tabular}

\vspace{15pt}
\noindent Describe the main issues to be addressed: goals, methodology and expected results. \textbf{The maximum length for this section is 1 page }(Verdana font size 10, letter size is suggested).
\vspace{10pt}

%Como consecuencia del crecimiento explosivo de la WEB y del uso de la redes sociales, la tarea de agrupamiento automático de texto resulta cada vez de mayor importancia debido a la necesidad de categorizar volúmenes crecientes de datos (e.g. Tweets y noticias). En varios escenarios, estos se generan de manera descentralizada, e.g. documentos recolectados por múltiples máquinas en el contexto de un motor de búsqueda\footnote{Ver por ejemplo como opera Google para indexar contenido textual en \url{https://www.google.com/insidesearch/howsearchworks/crawling-indexing.html} (01.08.2016)} incluso desde diversas zonas geográficas. La generación de grandes cantidades de documentos sobrepasa hoy en día las capacidades de computadores personales e incluso de equipos de cómputo de alto desempeño. Un enfoque para atacar el problema del gran volúmen de datos ha consistido en dividir la colección en diversas máquinas y aprovechar las capacidades de cómputo locales para obtener aquellos grupos o categorías temáticas que representan a la colección. De esta manera, primero se reduce el costo de almacenamiento al dividir la colección, y luego también se reduce el costo de cómputo al procesar una cantidad menor de datos en cada máquina, aprovechando incluso su capacidad de cómputo paralelo. Por último, el tamaño excesivo de las colecciones documentales hace impracticable su transmisión por la red a un sitio centralizado para su almacenamiento o análisis. El agrupamiento distribuido ataca este punto al identificar los grupos de la colección complete mediante la integración de los agrupamientos locales de cada máquina en un solo modelo. 

As a consequence of the explosive growth of the WEB and the used of social networks the task of automatic text clustering is becoming of greater importance due to the necessity to categorize increasing volumes of data (e.g. Tweets and News). In several scenarios, these are generated in a descentralized manner, e.g. collected documents by multiple machines in the context of search engines \footnote{See for example how does Google operate to index textual content in\url{https://www.google.com/insidesearch/howsearchworks/crawling-indexing.html} (01.08.2016)}, even in different geographical zones. The generation of large amounts of documents surpases nowadays the capacity of personal computers and even of hiperformance computers. One way to deal with the problem of large volumes of data consists in dividing the collection in different machines and then to seize the local computation capabilities to obtain those groups or categories that represent the document collection. By means of the latter, first the the cost of storage is decreased, and later also the computation cost is reduced, because each machine processes less amount of data, and even parallel approaches can be used. Finally, the large size of the document collection makes the transmission over the network of these data  to a centralized node for its storage and analysis, impractical. The Distributed Clustering attacks this point by identifying the groups of the complete collection by means of the integration of the local clusters created in each machine into a single model.

%que no se ha hecho
%Existen importantes contribuciones en la tarea de agrupamiento de datos distribuídos. Sin embargo, gran parte de estos esfuerzos ha considerado conjuntos de datos de baja dimensionalidad en su representación vectorial, alcanzando como máximo una cantidad de dimensiones del orden de las centenas. Para el caso del texto, su representación vectorial es naturalmente de alta dimensionalidad, debido al tamaño de los vocabularios (cantidad de palabras distintas en la colección). Esta característica hace difícil su procesamiento debido a que muchos métodos ampliamente usados (e.g. basados en K-Means o en densidad) escalan respecto de la cantidad de datos, pero su costo es cuadrático respecto de la cantidad de atributos del espacio de características.

Previously, important contributions to the Distributed Data Clustering task have been made. Nonetheless, a major portion of these efforts have been pointed to low dimensional datasets in terms of the sizes of their feature spaces (i.e. order of hundreds). In the text data scenario the vector representations built onto the Tf-Idf model and its variations, has a very high dimensional nature due to the size of the vocabulary, i.e. the number of different words employed in the contained documents. For
instance, a small text collection (e.g. order of thousands) can easily have a vocabulary of 10k words, which means that the feature space onto which the document vectors are spanned has that dimension. In addition to the high dimensionality, the sparsity involved in the computational representation of texts (only few words of the overall vocabulary appear in each document) puts a difficult challenge to traditional centralized clustering algorithms due to the main memory space required to process the data and
the detection of groups under distance measures that lose their effectiveness in high dimensional and sparse data.

% que se propone aquí
%En este proyecto se desarrollaran nuevas técnicas de agrupamiento de documentos de texto capaces de operar sobre colecciones documentales repartidas en varios nodos de una red de computadores, aprovechando además las capacidades de computo individuales de cada nodo. Para esto, los métodos a desarrollar deberan ser capaces de integrar en un solo modelo aquellas categorías de documentos identificadas en distintas máquinas. Por otra parte, deberan aprovechar el paralelismo existente en los procesadores de cada máquina y que además sean eficientes en términos de la cantidad de pasadas realizadas sobre cada sub-colección de documentos. Esto último, debido a que cada fragmento de la colección también puede tener un tamaño considerable, e.g. varias veces la cantidad de memoria RAM disponible en cada máquina.

In this project we propose to develop new Clustering techniques capable of dealing with text collections distributed into several computational nodes in a network and also capable of exploiting the computational power of each one. In order to do this the proposed methods should be able to combine in a central master node the partial models built in the other nodes. Furthermore, the algorithms must exploit the existing parallelism in each node either at CPU or GPU level and also
must be efficient in terms of the number of times each document is read and copied to main memory. This last trait is also very important since the subcollections processed in each node can also have a size several times larger than the main memory available.

%Para validar los métodos propuestos se utilizaran varias colecciones documentales usadas en problemas de clasificación de documentos, mejora de recuperación en motores de búsqueda y agrupamiento de texto. Para medir la calidad de los modelos generados se contrastarán aquellos obtenidos de manera distribuída con los obtenidos centralizadamente (en los casos que sea posible debido al tamaño de las colecciones), usando medidas estándar en agrupamiento tales como \textit{Rand score}, \textit{Mutual Information}, \textit{Homogeneity}, \textit{Completeness score} y la media armónica de estos últimos dos que corresponde a la \textit{V-Measure}. Las colecciones documentales disponibles no han sido diseñadas para probar algoritmos distribuídos, pero pueden ser fácilmente distribuídas en la validación como ya lo han hecho trabajos recientes.% (\citep{BEL13}).

In order to validate the proposed methods several real text collections used for Document Classification, Search Engine Evaluation and Document Clustering will be employed. To measure the performance attained by the proposed models several standard clustering measures will be computed over the obtained clusterings (e.g. \textit{Silhouette}, \textit{Adjusted Rand Index}, \textit{Adjusted Mutual Information score} and the \textit{V measure}). For collections with group-labels available external
measures will be computed and when no group labeling is available only internal measures will be computed. Finally, in order to provide a fair comparison among other methods and an objective evaluation random partitions across network nodes will be performed.


%\bibliographystyle{apalike}
% \bibliography{references}

\end{document}
